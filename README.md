# Using Falcon40B Instruct...and any other Open Source LLMs on GPU via HuggingFace  
In this tutorial we're going to be checking out some of the biggest baddest LLMs...but running them on a GPU!

## See it live and in action ğŸ“º
[![Tutorial](https://i.imgur.com/mSrUVse.jpg)](https://youtu.be/hMJgdVJWQRU 'Tutorial')

# Startup ğŸš€
1. Create a virtual environment `python -m venv gpullm`
2. Activate it: 
   - Windows:`.\gpullm\Scripts\activate`
   - Mac: `source gpullm/bin/activate`
3. Install PyTorch with CUDA Support 
N.B. I've included the lib in the requirements.txt file but this is latest installer as of creating this readme. 
`pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117`
Download and install CUDA (i've used 11.7 for this tutorial): https://developer.nvidia.com/cuda-11-7-0-download-archive
Download and install the matching cuDNN version ( v8.9.1): https://developer.nvidia.com/rdp/cudnn-archive 
5. Clone this repo `git clone https://github.com/nicknochnack/Falcon40B`
6. Go into the directory `cd Falcon40B`
7. Startup jupyter by running `jupyter lab` in a terminal or command prompt
8. Hit `Ctrl + Enter` to run through the notebook! 
10. Go back to my YouTube channel and like and subscribe ğŸ˜‰...no seriously...please! lol 

# Other References ğŸ”—
<p>-<a href="https://pytorch.org/get-started/locally/">PyTorch Installation</a>:main guide leveraged to handle GPU support.</p>
<p>-<a href="https://python.langchain.com/en/latest/modules/models/llms/integrations/huggingface_pipelines.html">Langchain HF Pipelines</a>:the HF Pipelines class is used in order to pass the local LLM to a chain.</p>
<p>-<a href="https://huggingface.co/tiiuae/falcon-40b-instruct">Falcon 40B Instruct Model Card</a>:check out the model details.</p>

# Who, When, Why?
ğŸ‘¨ğŸ¾â€ğŸ’» Author: Nick Renotte <br />
ğŸ“… Version: 1.x<br />
ğŸ“œ License: This project is licensed under the MIT License </br>

